% !TeX root=../tese.tex
%("dica" para o editor de texto: este arquivo é parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101

%% ------------------------------------------------------------------------- %%

\chapter{Referencial Teórico}\label{cap:fundamentacao}

\section{Engenharia de Software}
Segundo a definição do \citet{159342}, a engenharia de \emph{software} é a aplicação de uma sistemática, disciplinada e quantificável abordagem para o desenvolvimento, operação e manutenção de um \emph{software}. Para que a engenharia de \emph{software} seja viável, dada a complexidade dos sistemas demandados na atualidade, foram desenvolvidas etapas, metodologias e ferramentas que dessem suporte aos atores envolvidos no projeto, como desenvolvedores, analistas, investidores, clientes, entre outros.

\subsection{Etapas do Desenvolvimento de Software}
O Ciclo de Vida de Desenvolvimento de Software (SDLC) consiste numa sequência de processos pelos quais o desenvolvimento de um \emph{software} ocorre, de modo a produzir um resultado eficaz e de alta qualidade. Existe alguma variação no número de passos descritos por diferentes fontes, mas, em geral, há sete fases essenciais: planejamento, análise de requisitos, \emph{design}, codificação, testes, implantação e manutenção.

\subsubsection{Planejamento}

A fase inicial envolve definir o propósito e o escopo do \emph{software}. Durante esta etapa, a equipe de desenvolvimento deve levantar as tarefas necessárias, elaborar estratégias para cumpri-las e colaborar de modo a compreender as necessidades dos usuários finais. Neste processo, os objetivos do \emph{software} precisam ficar claros a todos os envolvidos.

Além disso, nesta fase ocorre o estudo de viabilidade, ou seja, desenvolvedores e outros atores do projeto avaliam desafios técnicos e financeiros que possam impactar a evolução e o sucesso do sistema. Ao fim desta fase, um plano de projeto é criado, com o intuito de detalhar as funções do sistema, os recursos necessários, possíveis riscos e o cronograma de execução. Ao definir papéis, responsabilidades e expectativas claras, o planejamento estabelece uma base sólida para um processo eficiente de desenvolvimento.

\subsubsection{Análise de Requisitos}

Nesta etapa, a equipe de projeto realiza o levantamento dos requisitos, por meio da coleta de informações das partes interessadas, como analistas, usuários e clientes. São empregadas técnicas como entrevistas, pesquisas e grupos de foco para compreender as necessidades e expectativas dos usuários.

Após a coleta, os dados são analisados, diferenciando os requisitos essenciais dos desejáveis. Essa análise possibilita a definição das funcionalidades, desempenho, segurança e interfaces do \emph{software}. Neste momento, são definidos os requisitos funcionais e não funcionais. Os requisitos funcionais especificam as funções que o \emph{software} deve ter, já os requisitos não funcionais tratam de como o sistema deve se comportar, incluindo aspectos como desempenho, segurança, usabilidade e escalabilidade.

O resultado desse processo é o Documento de Especificação de Requisitos (DER), que descreve o propósito, as funcionalidades e características do \emph{software}, servindo como guia para a equipe de desenvolvimento e fornecendo estimativas de custo. O êxito desta fase é crucial para o sucesso do projeto, pois assegura que a solução desenvolvida atenda às expectativas dos usuários.

\subsubsection{Design}

A fase de \emph{design} é responsável pela definição da estrutura do \emph{software}, abrangendo sua funcionalidade e aparência. A equipe de desenvolvimento detalha a arquitetura do sistema, a navegação, as interfaces de usuário e a modelagem do banco de dados, assegurando que o \emph{software} tenha boa usabilidade e seja eficiente.

Entre as atividades desta fase, destaca-se a elaboração de diagramas de fluxo de dados, de entidade-relacionamento, de classes, protótipos de interface e diagramas arquiteturais. O objetivo é garantir que as estruturas projetadas sejam suficientes para dar suporte a todas as funcionalidades do sistema. Também são identificadas dependências, pontos de integração e eventuais restrições, como limitações do equipamento físico e requisitos de desempenho.

O resultado desta fase é o Documento de \emph{Design} de Software (DDS) que estrutura formalmente as informações do projeto e trata preocupações de \emph{design}. Neste documento, são adicionados os artefatos produzidos, servindo como guia estável para coordenar equipes grandes e garantir que todos os componentes do sistema funcionem de maneira integrada.

\subsubsection{Codificação}

Na fase de codificação, os engenheiros e desenvolvedores transformam o \emph{design} do \emph{software} em código executável. O objetivo é produzir um \emph{software} funcional, eficiente e com boa usabilidade. Para isso, são utilizadas linguagens de programação adequadas, seguindo o DDS e diretrizes de codificação estabelecidas pela organização e pela legislação local.

Durante esta fase, são realizadas revisões de código, nas quais os membros da equipe examinam o trabalho uns dos outros para identificar erros ou inconsistências, garantindo elevados padrões de qualidade. Além disso, testes preliminares internos são conduzidos para garantir que as funcionalidades básicas do sistema sejam atendidas.

Ao final da fase de codificação, o \emph{software} passa a existir como um produto funcional, representando a materialização dos esforços das etapas anteriores, mesmo que ainda sejam necessários refinamentos e ajustes subsequentes. O resultado desta fase é o código-fonte.

\subsubsection{Testes}

A fase de testes consiste em verificar a qualidade e a confiabilidade do \emph{software} antes de sua entrega aos usuários finais. Seu objetivo é identificar falhas, erros e vulnerabilidades, assegurando que o sistema atenda aos requisitos especificados.

Inicialmente, são definidos parâmetros de teste alinhados aos requisitos do \emph{software} e casos de teste que contemplem diferentes cenários de uso. Em seguida, são conduzidos testes de diversos níveis e tipos, incluindo testes de unidade, de integração, de sistema, de segurança e de aceitação, permitindo a avaliação tanto de componentes individuais quanto da operação do sistema na sua totalidade.

Quando um erro é identificado, ele é registrado detalhadamente, incluindo seu comportamento, métodos de reprodução e impacto sobre o sistema. As falhas são encaminhadas para correção e o \emph{software} retorna à fase de testes para validação. Este ciclo de teste e correção se repete até que o sistema esteja conforme os critérios previamente estabelecidos. O resultado desta fase é um código-fonte mais robusto e menos propenso a falhas.

\subsubsection{Implantação}

A fase de implantação ou \emph{deployment} consiste em disponibilizar o \emph{software} aos usuários finais, garantindo sua operacionalidade no ambiente de produção. Este processo ocorre tanto no primeiro lançamento do sistema, quanto quando ele já está em uso pelos usuários e passando por atualizações, neste caso, o processo deve minimizar interrupções e impactos no acesso dos usuários.

Além de colocar o \emph{software} em operação, esta fase envolve assegurar que os usuários compreendam seu funcionamento. Para isso, podem ser fornecidos manuais, treinamentos e suporte técnico. Desta maneira, a fase de implantação marca a transição do \emph{software} de projeto para produto, iniciando efetivamente o cumprimento de seus objetivos e a entrega de valor ao usuário.

\subsubsection{Manutenção}

A fase de manutenção é caracterizada por suporte contínuo e por melhorias incrementais, de modo a garantir que o \emph{software} mantenha seu funcionamento adequado, acompanhe as necessidades dos usuários e as demandas de mercado. Nesta fase, são realizadas atualizações, correções de falhas e suporte ao usuário. Considerando o horizonte de longo prazo, a manutenção inclui estratégias de modernização ou substituição do \emph{software}, buscando manter sua relevância e adequação às evoluções tecnológicas.

\subsection{Ferramentas de Desenvolvimento}

As ferramentas de desenvolvimento de \emph{software} oferecem suporte às etapas do SDLC. O uso combinado dessas ferramentas contribui para maior produtividade, qualidade e confiabilidade no desenvolvimento de \emph{software}. Elas incluem ferramentas de:

\begin{itemize}
    \item \textbf{Controle de Versão (como \emph{Git} e \emph{SVN}):} permitem registrar e gerenciar alterações no código-fonte temporalmente, possibilitando a colaboração simultânea entre desenvolvedores, a recuperação de versões anteriores e o rastreamento completo do histórico de mudanças;

    \item \textbf{Ambientes de Desenvolvimento Integrados (IDEs) (como \emph{Visual Studio}, \emph{IntelliJ IDEA} e \emph{Eclipse}):} oferecem um conjunto de ferramentas em um único ambiente, incluindo edição de código, depuração, testes, gerenciamento de dependências, integração com sistemas de controle de versão e ferramentas de IA generativa;

    \item \textbf{Gerenciamento de Projetos (como \emph{Jira}, \emph{Trello} e \emph{Asana}):} auxiliam na organização e priorização de tarefas, acompanhamento do progresso e comunicação entre membros da equipe, fornecendo transparência e facilitando a coordenação do trabalho;

    \item \textbf{Integração e Entrega Contínua (CI/CD) (como \emph{Jenkins}, \emph{GitHub Actions} e \emph{GitLab CI}):} automatizam processos de compilação, testes e implantação, promovendo maior qualidade e agilidade nas entregas de \emph{software};

    \item \textbf{Teste (como \emph{Selenium}, \emph{JUnit} e \emph{Postman}):} permitem a execução de testes automatizados e manuais para validar funcionalidades, desempenho e segurança do sistema, contribuindo para a detecção precoce de falhas e a melhoria da qualidade do \emph{software}.

    \item \textbf{Virtualização e Monitoramento (como \emph{Docker}, \emph{Prometheus} e \emph{Grafana}):} permitem criar ambientes isolados e consistentes para execução do \emph{software}, garantindo que ele funcione de maneira idêntica em diferentes máquinas. Além disso, possibilitam acompanhar o desempenho e a saúde dos sistemas em produção, auxiliando na detecção precoce de problemas na qualidade do \emph{software}.
\end{itemize}

\section{Inteligência Artificial}
Inteligência Artificial (IA) é o campo da ciência da computação que se dedica a criar sistemas capazes de executar tarefas que normalmente exigiriam inteligência humana, como reconhecimento de padrões, raciocínio, tomada de decisão, resolução de problemas e aprendizado a partir de dados. Segundo uma declaração da IEEE \citep{ieee2019ai}, a inteligência artificial inclui tecnologias computacionais inspiradas no modo como pessoas e outros organismos biológicos percebem, aprendem, raciocinam e agem.

As aplicações de IA afetam cada vez mais diversos aspectos da sociedade, incluindo defesa e segurança nacional, sistemas de justiça, comércio, finanças, manufatura, saúde, transporte, educação, entretenimento e interações sociais. Aplicações como essas estão se expandindo pela combinação de processadores avançados, grandes volumes de dados e novos algoritmos. Segundo um estudo da \citet{mckinsey2018-ai-impact}, a IA contribuirá com cerca de 13 trilhões de dólares para o PIB global até 2030.

\subsection{Aprendizado de Máquina}

Aprendizado de máquina (\emph{Machine Learning}) é um subcampo da IA que se concentra em criar algoritmos capazes de aprender e fazer previsões a partir de quantidades expressivas de dados, geralmente estruturados ou rotulados, sem serem explicitamente programados para cada tarefa. Um conceito importante para o aprendizado de máquina são as redes neurais, modelos computacionais inspirados na estrutura do cérebro humano, compostas por camadas de nós (neurônios artificiais) interconectados, capazes de processar informações e aprender padrões a partir de exemplos. Redes neurais são amplamente utilizadas em tarefas como classificação, reconhecimento de imagens e processamento de linguagem natural.

% TODO - continuar correções desse trecho

\subsubsection{Aprendizado Profundo}

Aprendizado Profundo, como um subconjunto do Aprendizado de Máquina, distingue-se pelo uso de Redes Neurais Artificiais (RNAs), estruturas computacionais inspiradas no funcionamento do cérebro humano. Essas redes são compostas por camadas de nós interconectados, onde cada nó é responsável por aprender um atributo específico dos dados. O termo "profundo" se origina da utilização de múltiplas camadas ocultas entre a camada de entrada e a de saída, o que permite que a rede construa representações hierárquicas e complexas dos dados.

O processo de treinamento de uma rede neural profunda envolve a alimentação de um grande conjunto de dados rotulados. Conforme a rede processa esses dados, ela ajusta os "pesos" nas conexões entre os nós, usando técnicas como o

O processo de treinamento de uma rede neural profunda envolve a alimentação de um grande conjunto de dados. Conforme a rede processa esses dados, ela ajusta os "pesos" nas conexões entre os nós, utilizando o algoritmo de \emph{backpropagation} para minimizar o erro e melhorar sua precisão. A capacidade das redes neurais profundas de reter ou esquecer informações seletivamente, processar grandes volumes de dados e realizar tarefas complexas, como processamento de linguagem natural e reconhecimento de fala, as torna uma das tecnologias mais poderosas da IA contemporânea.

Existem arquiteturas de redes neurais profundas especializadas para diferentes tipos de dados. As Redes Neurais Convolucionais (CNNs), por exemplo, são particularmente eficazes para processar imagens e vídeos. Elas utilizam camadas convolucionais para extrair informações dos dados de entrada, camadas de agrupamento para reduzir a dimensionalidade e camadas totalmente conectadas para fazer previsões de alto nível. As Redes Neurais Recorrentes (RNNs) são projetadas para dados sequenciais, como texto ou séries temporais. Sua arquitetura permite que a rede combine a entrada atual com um estado oculto anterior, conferindo-lhe uma forma de "memória" para capturar dependências e padrões ao longo do tempo. Uma forma avançada de RNN é a memória de curto prazo longa (LSTM), que aprimora a capacidade da rede de "lembrar" o que aconteceu em camadas anteriores, tornando-a ideal para tarefas de linguagem natural e análise de sentimentos.

\subsection{IA Generativa}

A Inteligência Artificial Generativa, ou IA Generativa, é um subconjunto da IA que desenvolveu a capacidade de criar conteúdos novos e originais, como texto, imagens, vídeos, áudio e código, em resposta a prompts de texto, que podem variar de simples a altamente complexos. Este campo emergiu como um dos mais proeminentes da IA nos últimos anos, marcando uma transição da capacidade da IA de apenas processar e classificar dados para a de criar ativamente.

O avanço da IA Generativa está intimamente ligado à inovação arquitetural dos modelos de transformação, ou \emph{Transformers}. Antes de sua ascensão, as Redes Neurais Recorrentes (RNNs) eram a tecnologia de ponta para processamento de dados sequenciais. No entanto, sua natureza sequencial impedia a paralelização e limitava o treinamento em grandes conjuntos de dados. A arquitetura \emph{Transformer}, introduzida no artigo de 2017 \emph{“Attention is All You Need”} por Vaswani et al., revolucionou este processo ao permitir que a rede processasse uma sequência de dados inteira simultaneamente. Essa capacidade de paralelização foi a tecnologia fundamental que permitiu o treinamento de modelos em conjuntos de dados massivos, utilizando o poder de processamento das GPUs.

Essa inovação arquitetural levou ao desenvolvimento dos Grandes Modelos de Linguagem (LLMs), que são modelos de linguagem pré-treinados em massivas quantidades de texto e que se tornaram a base da maioria das aplicações modernas de \emph{embedding} de palavras. Exemplos notáveis de LLMs incluem o BERT do Google (que usa o codificador do \emph{Transformer}) e a série GPT da OpenAI (que usa o decodificador). A capacidade dos LLMs de gerar texto coerente e de alta qualidade foi o que, de fato, catalisou a era moderna da IA Generativa.

A explosão da IA Generativa que se observa hoje não é um evento isolado, mas um resultado direto de uma inovação arquitetural específica. A capacidade de paralelização (a causa) permitiu o treinamento em escala sem precedentes (o efeito imediato), que, por sua vez, desbloqueou a capacidade de gerar conteúdo novo e coerente (o efeito subsequente). Portanto, o sucesso da IA Generativa é um triunfo da engenharia e escalabilidade tanto quanto do algoritmo em si.

As aplicações da IA Generativa se estendem por diversas modalidades. Na geração de texto, os LLMs impulsionam chatbots, assistentes virtuais e ferramentas de geração de código. Na geração de imagens e vídeos, ferramentas como o Adobe Firefly permitem aos usuários criar conteúdo visual a partir de comandos de texto. O processo é intuitivo: o usuário insere um comando de texto detalhado, e o modelo de IA, que utiliza Aprendizado de Máquina e Processamento de Linguagem Natural, converte a descrição em um clipe de vídeo profissional, geralmente com cinco segundos de duração.

\subsubsection{Modelos de Linguagem}

Modelos de linguagem são algoritmos estatísticos e computacionais projetados para compreender, gerar e manipular texto em linguagem natural. O objetivo central desses modelos é capturar padrões linguísticos, tanto de natureza sintática quanto semântica, a partir de grandes coleções de dados textuais (\emph{corpora}). A partir desses padrões, o modelo consegue prever a probabilidade de uma palavra ocorrer em determinada sequência, construir sentenças coerentes, interpretar perguntas e até mesmo inferir significados implícitos no discurso.

Historicamente, os primeiros modelos de linguagem eram baseados em abordagens probabilísticas relativamente simples, como modelos de \emph{n-gramas}, que calculavam a probabilidade de uma palavra com base nas anteriores. Com o avanço da aprendizagem profunda (\emph{deep learning}), redes neurais recorrentes (RNNs) e arquiteturas como LSTMs (\emph{Long Short-Term Memory}) e GRUs (\emph{Gated Recurrent Units}) passaram a ser empregadas para capturar dependências de longo prazo em sequências textuais. Apesar disso, tais arquiteturas enfrentavam dificuldades em lidar com textos extensos devido a problemas como o desvanecimento do gradiente.

\subsubsection{Transformers}

Introduzidos em 2017 por Vaswani et al. no artigo \emph{“Attention is All You Need”}, os \emph{Transformers} representam uma das arquiteturas mais influentes e transformadoras no campo do processamento de linguagem natural (PLN). Diferentemente de RNNs e LSTMs, os Transformers não processam o texto de forma sequencial; em vez disso, utilizam mecanismos de \emph{atenção} que permitem analisar todas as palavras de uma sequência em paralelo. Isso possibilita capturar relações contextuais entre termos que estão distantes no texto, algo que modelos anteriores realizavam de forma limitada e ineficiente.

O componente central dessa arquitetura é o mecanismo de \emph{self-attention}, que calcula pesos relacionando cada palavra da sequência com todas as outras, permitindo ao modelo focar seletivamente nas informações mais relevantes para determinada tarefa. Essa capacidade de processamento paralelo e contextualização eficiente revolucionou não apenas o PLN, mas também áreas como visão computacional e bioinformática, sendo hoje a base de modelos de ponta em inteligência artificial.

\subsubsection{Modelos de Linguagem de Grande Escala (\emph{Large Language Models -- LLMs})}

Modelos de linguagem de grande escala (\emph{Large Language Models} – LLMs) são uma evolução direta da arquitetura Transformer, caracterizados pelo uso de quantidades massivas de dados de treinamento e pela presença de bilhões (ou até trilhões) de parâmetros ajustáveis. Essa escala permite que os modelos capturem nuances linguísticas extremamente sofisticadas, possibilitando não apenas a geração de texto coerente, mas também a realização de tarefas complexas como tradução automática, sumarização de documentos, resposta a perguntas, geração de código e até raciocínio lógico em determinados contextos.

Entre os exemplos mais representativos de LLMs estão o \emph{GPT} (Generative Pre-trained Transformer), da OpenAI, o \emph{BERT} (Bidirectional Encoder Representations from Transformers), do Google, e o \emph{T5} (Text-to-Text Transfer Transformer). Cada um desses modelos introduziu contribuições significativas: o GPT popularizou a abordagem de pré-treinamento seguido de ajuste fino para múltiplas tarefas; o BERT trouxe o treinamento bidirecional, permitindo maior compreensão de contexto; e o T5 propôs unificar diferentes tarefas de PLN no paradigma \emph{text-to-text}.

Devido ao seu poder e versatilidade, os LLMs vêm sendo amplamente aplicados em produtos tecnológicos, assistentes virtuais, motores de busca e sistemas de suporte à decisão. Entretanto, seu uso também levanta desafios importantes relacionados a viés nos dados, consumo energético elevado durante o treinamento e riscos de uso inadequado, tornando essencial a discussão ética e responsável sobre seu desenvolvimento e aplicação.

% TODO - adicionar explicação sobre métricas de código (churn)
% TODO - adicionar explicação sobre boas práticas, incluindo DRY